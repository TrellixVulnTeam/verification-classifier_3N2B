{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background:\n",
    "\n",
    "Following on this github issue (https://github.com/livepeer/research/issues/17), particularly on the Video Quality Checking topic:\n",
    "> There are various easily checkable properties that can be extracted from the video itself such as the codec, the resolution, timestamps, and perhaps certain other bitstream features. However those properties alone do not ensure an important aspect of verification: that the transcoded content itself is a reasonable match for the original source given a good-faith effort at transcoding.\n",
    "\n",
    "> What is a \"reasonable match\" and what is a \"good-faith effort at transcoding\"? Some problems with the video may include:\n",
    "   -  Watermarking or other manipulation of the source content\n",
    "   -  Uncalled for resolution changes mid-stream\n",
    "   -  Excessive frame dropping\n",
    "   -  Low quality encoder or inappropriate encoding settings\n",
    "\n",
    "> What criteria should we be checking addition to video quality?\n",
    "   -  Codec and container itself\n",
    "   -  Timestamps\n",
    "   -  Any metadata?\n",
    "\n",
    "\n",
    "The aim of this notebook is to give an answer to the following set of questions raised in the context of finding an objective metric to assess the quality of stream videos:\n",
    "\n",
    "-  How to use these per-frame scores for video\n",
    "-  How to incorporate these scores into a pass/fail classifier?\n",
    "-  What does each contribute towards the classifier?\n",
    "-  How are these affected by variations in input and output?\n",
    "-  How is verification affected if either metric is removed from the equation?\n",
    "-  Can we extrapolate the behavior of these metrics across unknown inputs? What are the boundaries?\n",
    "-  What is the performance/computational impact of incorporating these metrics into the classifier? Can this be run online within our sub-2s latency budget?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology:\n",
    "\n",
    "We have created a dataset with 140 videos collected from the YT8M dataset using our tool from previous research (https://github.com/epiclabs-io/YT8M).\n",
    "\n",
    "These 140 are basically those with a set of at least four renditions codified in H264 (240, 360, 480, 720 and 1080).\n",
    "In order to depart from the null hypothesis that video is transcoded correctly, this seems like a very good assumption as it is in YouTube's own interest to be so.\n",
    "\n",
    "In order to extend the list of possible metrics, we have researched a few candidates based on the perceptual hashing of the original and the codified videos: cosine distance, euclidean distance and Hamming distance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries and initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import plotly.plotly as py\n",
    "import plotly.offline as offline\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "from matplotlib.mlab import PCA\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve data, group it and normalize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retrieve data from repo\n",
    "metrics_df = pd.read_csv('../output/metrics.csv')\n",
    "metrics_df = metrics_df.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "display(metrics_df.head(5))\n",
    "#metrics_df = metrics_df.drop(metrics_df[metrics_df['temporal_difference-cosine']>0.10].index, axis=0)\n",
    "metrics_df['title'] = metrics_df['level_0']\n",
    "\n",
    "attack_series = []\n",
    "for _, row in metrics_df.iterrows():\n",
    "    attack_series.append(row['level_1'].split('/')[-2])\n",
    "    \n",
    "metrics_df['attack'] = attack_series\n",
    "metrics_df['temporal_canny-cross-correlation'] = metrics_df['temporal_canny-cross-correlation'].str.replace(']','')\n",
    "metrics_df['temporal_canny-cross-correlation'] = metrics_df['temporal_canny-cross-correlation'].str.replace('[','')\n",
    "metrics_df['temporal_canny-cross-correlation'] = pd.to_numeric(metrics_df['temporal_canny-cross-correlation'])\n",
    "\n",
    "metrics_df['temporal_dct-cross-correlation'] = metrics_df['temporal_dct-cross-correlation'].str.replace(']','')\n",
    "metrics_df['temporal_dct-cross-correlation'] = metrics_df['temporal_dct-cross-correlation'].str.replace('[','')\n",
    "metrics_df['temporal_dct-cross-correlation'] = pd.to_numeric(metrics_df['temporal_dct-cross-correlation'])\n",
    "\n",
    "metrics_df['temporal_histogram_distance-cross-correlation'] = metrics_df['temporal_histogram_distance-cross-correlation'].str.replace(']','')\n",
    "metrics_df['temporal_histogram_distance-cross-correlation'] = metrics_df['temporal_histogram_distance-cross-correlation'].str.replace('[','')\n",
    "metrics_df['temporal_histogram_distance-cross-correlation'] = pd.to_numeric(metrics_df['temporal_histogram_distance-cross-correlation'])\n",
    "\n",
    "metrics_df['temporal_cross_correlation-cross-correlation'] = metrics_df['temporal_cross_correlation-cross-correlation'].str.replace(']','')\n",
    "metrics_df['temporal_cross_correlation-cross-correlation'] = metrics_df['temporal_cross_correlation-cross-correlation'].str.replace('[','')\n",
    "metrics_df['temporal_cross_correlation-cross-correlation'] = pd.to_numeric(metrics_df['temporal_cross_correlation-cross-correlation'])\n",
    "\n",
    "display(metrics_df.head(25))\n",
    "display(metrics_df.describe())\n",
    "# Filter out those rows we might not be interested about by dropping them from the dataframe\n",
    "# metrics_df = metrics_df.drop(metrics_df[metrics_df['temporal_canny-cross-correlation'] > 100].index, axis=0)\n",
    "# metrics_df = metrics_df.drop(metrics_df[metrics_df['temporal_canny-euclidean'] > 20].index, axis=0)\n",
    "metrics_df = metrics_df.drop(metrics_df[metrics_df['attack'].str.contains('rotate')].index, axis=0)\n",
    "metrics_df = metrics_df.drop(metrics_df[metrics_df['attack'].str.contains('flip')].index,axis=0)\n",
    "metrics_df = metrics_df.drop(metrics_df[metrics_df['attack'].str.contains('black')].index,axis=0)\n",
    "metrics_df = metrics_df.drop(metrics_df[metrics_df['attack'].str.contains('vignette')].index,axis=0)\n",
    "\n",
    "# Group values for each title\n",
    "metrics = ['temporal_canny-mean',\n",
    "           'temporal_canny-cross-correlation',\n",
    "           'temporal_canny-euclidean',\n",
    "           'temporal_canny-std',\n",
    "           'temporal_dct-mean', \n",
    "           'temporal_dct-cross-correlation', \n",
    "           'temporal_dct-euclidean', \n",
    "           'temporal_dct-std', \n",
    "           'temporal_histogram_distance-mean',\n",
    "           'temporal_histogram_distance-cross-correlation',\n",
    "           'temporal_histogram_distance-euclidean',\n",
    "           'temporal_histogram_distance-std',\n",
    "           'temporal_cross_correlation-mean',\n",
    "           'temporal_cross_correlation-cross-correlation',\n",
    "           'temporal_cross_correlation-euclidean',\n",
    "           'temporal_cross_correlation-std',\n",
    "           'vmaf']\n",
    "\n",
    "grouped_df = metrics_df.groupby(['level_0'] + ['level_1'] + metrics + ['attack'], as_index=False).count()\n",
    "grouped_df = grouped_df.sort_values(by=['attack'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colors(n):\n",
    "    ret = []\n",
    "    r = int(random.random() * 256)\n",
    "    g = int(random.random() * 256)\n",
    "    b = int(random.random() * 256)\n",
    "    step = 256 / n\n",
    "    for i in range(n):\n",
    "        r += step\n",
    "        g += step\n",
    "        b += step\n",
    "        r = int(r) % 256\n",
    "        g = int(g) % 256\n",
    "        b = int(b) % 256\n",
    "        ret.append((r,g,b)) \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a plot from columns of a pandas DataFrame\n",
    "def plot_metric(metric, dataframe, column='attack', contours=True, type='linear'):\n",
    "    x_data = dataframe[column]\n",
    "    y_data = dataframe[metric]\n",
    "    data = []\n",
    "    assets_list = dataframe['level_0'].unique()\n",
    "    attacks_list = dataframe['attack'].unique()\n",
    "    \n",
    "\n",
    "    count = 0\n",
    "    for asset in assets_list:\n",
    "        color_list = colors(len(assets_list))\n",
    "        trace1 = go.Histogram2dContour(x=x_data[dataframe['level_0']==asset],\n",
    "                                       y=y_data[dataframe['level_0']==asset],\n",
    "                                       name=metric\n",
    "                                       )\n",
    "\n",
    "        [r,g,b] = color_list[count]\n",
    "        trace2 = go.Scatter(x=x_data[dataframe['level_0']==asset],\n",
    "                            y=y_data[dataframe['level_0']==asset],\n",
    "                            mode='markers',\n",
    "                            marker=dict(color=['rgba({}, {}, {}, .9)'.format(r,g,b) for attack in attacks_list],\n",
    "                                        size=5,\n",
    "                                        opacity=1),\n",
    "                            text=asset,\n",
    "                            name=asset,\n",
    "                            hoverinfo='text')\n",
    "        if contours:\n",
    "            data.append(trace1)\n",
    "            data.append(trace2)\n",
    "        else:\n",
    "            data.append(trace2)\n",
    "        count += 1\n",
    "        \n",
    "    layout = {\n",
    "        \"height\":700, \n",
    "            \"width\":800,\n",
    "            \"title\": \"Video metrics disparity: {}\".format(metric), \n",
    "            \"xaxis\": {\"title\": \"Rendition\", \"type\": \"category\", \"automargin\": True, \"tickangle\":60 }, \n",
    "            \"yaxis\": {\"title\": \"Metric value\", \"type\": type},\n",
    "            \"hovermode\":\"closest\",\n",
    "            \"margin\": {\"b\":100},\n",
    "            \"showlegend\": False\n",
    "            }\n",
    "    \n",
    "    \n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Iterate through each metric and obtain the respective charts\n",
    "for metric in metrics:\n",
    "    plot_metric(metric, grouped_df, contours=False, type='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot charts with logarithmic scale\n",
    "\n",
    "plot_metric('temporal_dct-mean', grouped_df, contours=False, type='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spearman's correlation coefficient\n",
    "Seek potential correlations between metrics. The intensity of this correlations is given by the Spearman’s correlation coefficient displayed in the table below. Without entering into details, let’s explain that Spearman’s correlation coefficient gives the same information as that of Pearson’s, but calculated on ranks instead of actual data values. This allows for identification of both positive (blue) and negative (red) correlations, where +1 means total positive correlation (when one feature grows, so does the other) and -1 means total negative correlation (when one feature grows, the other decreases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = grouped_df[metrics]\n",
    "corr = data.corr('spearman')\n",
    "corr.style.background_gradient().set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df[metrics].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter matrix between metrics\n",
    "Matrix below depicts a pairs plot of our newly generated dataset. It builds on two elementary plots: scatter plots of one metric against each other and histograms of themselves in the diagonal. We can see that all distances (euclidean, cosine and Hamming) are linearly related, meaning basically that they could be used almost interchangeably. On the other hand, SSIM and PSNR are also somehow correlated in a logarithmic / exponential manner, but inversely with regards to the hash distance metrics. In a world apart, the more sophisticated MS-SSIM and VMAF present some degree of connection between them, and display a similar pattern as SSIM in their lower bound with regard to the hash distance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter = scatter_matrix(metrics_df[metrics], alpha=0.2, figsize=(16,16), diagonal='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISCUSSION\n",
    "\n",
    "## Cosine distance\n",
    "\n",
    "This distance is measured over the hash created by reducing a grayscale version (luminance space) of each frame (original and codified) to a 16x16 pixel image, then obtaining a 15 bit hash and computing their cosine distance.\n",
    "\n",
    "## Euclidean distance\n",
    "\n",
    "This distance is measured over the hash created by reducing a grayscale version (luminance space) of each frame (original and codified) to a 16x16 pixel image, then obtaining a 15 bit hash and computing their euclidean distance.\n",
    "\n",
    "## Hamming distance\n",
    "This distance is measured over the hash created by reducing a grayscale version (luminance space) of each frame (original and codified) to a 16x16 pixel image, then obtaining a 15 bit hash and computing their euclidean distance.\n",
    "\n",
    "## PSNR\n",
    "PSNR is computed over the grayscale of both reference and codified using the psnr function of the ffmpeg framework. See the Tools notebook and its associated scripts:\n",
    "* https://github.com/livepeer/verification-classifier/blob/master/notebooks/Tools.ipynb\n",
    "* https://github.com/livepeer/verification-classifier/blob/master/scripts/shell/evaluate-psnr-ssim.sh\n",
    "\n",
    "## SSIM\n",
    "SSIM is obtained using ssim function of the ffmpeg framework. See the Tools notebook and its associated scripts\n",
    "* https://github.com/livepeer/verification-classifier/blob/master/notebooks/Tools.ipynb\n",
    "* https://github.com/livepeer/verification-classifier/blob/master/scripts/shell/evaluate-psnr-ssim.sh\n",
    "\n",
    "## MS-SSIM\n",
    "This metric is obtained using ms-ssim function of the libav framework. See the Tools notebook and its associated scripts: \n",
    "* https://github.com/livepeer/verification-classifier/blob/master/notebooks/Tools.ipynb\n",
    "* https://github.com/livepeer/verification-classifier/blob/master/scripts/shell/evaluate-ms-ssim.sh\n",
    "\n",
    "\n",
    "## VMAF\n",
    "This metric is obtained using the libvmaf command from Netflix. See the Tools notebook and its associated scripts: \n",
    "* https://github.com/livepeer/verification-classifier/blob/master/notebooks/Tools.ipynb\n",
    "* https://github.com/livepeer/verification-classifier/blob/master/scripts/shell/evaluate-vmaf.sh\n",
    "\n",
    "\n",
    "## Temporal difference Euclidean\n",
    "This metric is obtained as the Euclidean distance between two vectors obtained from the original and its rendition videos, respectively, in a No Reference manner. First, the time series of the original is computed by subtracting the pixels of successive frames form their next. The same is applied on each rendition. The distances between the created pairs of vectors are then obtained. See implementation in the video_asset_processor.py module:\n",
    "* https://github.com/livepeer/verification-classifier/blob/master/scripts/video_asset_processor.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSIONS\n",
    "\n",
    "As it is explained here: https://link.springer.com/article/10.1007/s11042-017-4831-6,\n",
    "the metrics above can be categorized as objective Full Reference metrics for Video Quality Assessment.\n",
    "We would reccomend to use other type of metrics, as described in the above reference, and use an unsupervised approach given the complexity of the problem at hand.\n",
    "\n",
    "#### How to use PSNR / SSIM (per-frame scores) for video (sequence of frames)?\n",
    "\n",
    "Good characterization of time series is achieved by wisely using statistical techniques (average, histograms, wavelets, etc.) that can summarize the properties of a sequence. In this case we have extracted the mean value for all metrics.\n",
    "However, the main issue will not be just the extrapolation of a per frame metric (PSNR, MS-SSIM, MSE, entropy, etc.) to a time series sequence of frames.\n",
    "The most complicated part comes when we need to define what is a \"good\" configuration of those time series.  \n",
    "\n",
    "#### How to incorporate these scores into a pass/fail classifier? What does each contribute towards the classifier?\n",
    "\n",
    "As pointed out above, by simple measurement of different instantaneous metrics there is no way one can do this. We could potentially define acceptable thresholds for that ratio for given renditions and bitrates / encoding parameters, but they would remain arbitrary.\n",
    "\n",
    "#### How are these affected by variations in input and output?\n",
    "\n",
    "In the figures above, we can appreciate that different sequences of frames (140 assets extracted from YouTube) with different configurations give slightly different results, although SSIM seems to be the one with the least dispersion.\n",
    "\n",
    "#### How is verification affected if either metric is removed from the equation? Can we extrapolate the behavior of these metrics across unknown inputs? What are the boundaries?\n",
    "\n",
    "As it is shown in the charts, the mean values for each metric are sensitive to different kinds of encoding and video characteristics. This leads one to think that (as it is explained in literature) different metrics are sensitive to different kinds of inputs. This renders them unsuitable for actual training of a supervised machine learning algorithm, given the amplitude of the space of possible ground truths.\n",
    "\n",
    "\n",
    "#### What is the performance/computational impact of incorporating these metrics into the classifier? Can this be run online within our sub-2s latency budget?\n",
    "\n",
    "\n",
    "Figure below shows the (more or less) constant elapsed time required to compute both PSNR and SSIM between the original frame and the frames generated at 500Kbps and 250Kbps.\n",
    "\n",
    "This gives an overhead (in our implementation, using skimage's compare_ssim) of about 0.172s per frame (=4s for 24 frames) for Big Buck Bunny.\n",
    "\n",
    "Obviously, there is no particular need to catch every single frame, and more efficient implementations could be found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
