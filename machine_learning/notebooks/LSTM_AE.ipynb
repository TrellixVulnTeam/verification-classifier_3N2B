{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as offline\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import signal\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import LSTM, RepeatVector, TimeDistributed, Dense, Dropout, Input\n",
    "from keras.optimizers import Adam \n",
    "\n",
    "offline.init_notebook_mode(connected=True)\n",
    "\n",
    "sys.path.insert(0, '../../scripts/modeling_toolbox/')\n",
    "# load the autoreload extension\n",
    "%load_ext autoreload\n",
    "# Set extension to reload modules every time before executing code\n",
    "%autoreload 2\n",
    "\n",
    "from metric_processor import MetricProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(network_history, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.semilogy(network_history.history['loss'])\n",
    "    plt.semilogy(network_history.history['val_loss'])\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def print_scores(mse_train, mse_test, mse_attacks):\n",
    "    th = np.quantile(np.mean(train_, axis=1), 0.99)\n",
    "\n",
    "    mse_train = np.mean(train_, axis=1)\n",
    "    mse_test = np.mean(test_, axis=1)\n",
    "    mse_attacks = np.mean(attacks_, axis=1)\n",
    "\n",
    "    print('Thresholding the 99% quantile')\n",
    "    print('Train TPR: {}'.format(1 - sum(np.array(mse_train) > th) / len(mse_train)))\n",
    "    print('Test TPR: {}'.format(1 - sum(np.array(mse_test) > th) / len(mse_test)))\n",
    "    print('TNR: {}'.format(1 - sum(np.array(mse_attacks) < th) / len(mse_attacks)))\n",
    "\n",
    "    true_positives = sum(np.array(mse_test) < th)\n",
    "    false_negatives = sum(np.array(mse_test) > th)\n",
    "    false_positives = sum(np.array(mse_attacks) < th)\n",
    "    true_negatives = sum(np.array(mse_attacks) > th)\n",
    "\n",
    "    beta = 20\n",
    "    precision = true_positives/(true_positives+false_positives)\n",
    "    recall = true_positives/(true_positives+false_negatives)\n",
    "    F20 = (1 + (beta ** 2))*precision*recall/((beta ** 2)*precision + recall)\n",
    "    print('F20: {}'.format(F20))\n",
    "\n",
    "    print('\\n-------------------\\n')\n",
    "\n",
    "    th = np.quantile(mse_test, 0.999)\n",
    "    print('Thresholding the 99.9% quantile')\n",
    "    print('Train TPR: {}'.format(1 - sum(np.array(mse_train) > th) / len(mse_train)))\n",
    "    print('Test TPR: {}'.format(1 - sum(np.array(mse_test) > th) / len(mse_test)))\n",
    "    print('TNR: {}'.format(1 - sum(np.array(mse_attacks) < th) / len(mse_attacks)))\n",
    "\n",
    "    true_positives = sum(np.array(mse_test) < th)\n",
    "    false_negatives = sum(np.array(mse_test) > th)\n",
    "    false_positives = sum(np.array(mse_attacks) < th)\n",
    "    true_negatives = sum(np.array(mse_attacks) > th)\n",
    "\n",
    "    beta = 20\n",
    "    precision = true_positives/(true_positives+false_positives)\n",
    "    recall = true_positives/(true_positives+false_negatives)\n",
    "    F20 = (1 + (beta ** 2))*precision*recall/((beta ** 2)*precision + recall)\n",
    "    print('F20: {}'.format(F20))\n",
    "    \n",
    "def print_series(train_, train_re, test_, test_re, attacks_, attacks_re):\n",
    "    \n",
    "    f, axs = plt.subplots(3,5,figsize=(20,15))\n",
    "\n",
    "    ims = np.random.randint(0,3000,5)\n",
    "\n",
    "    axs[0,0].plot(train_[ims[0]], label=\"train\")\n",
    "    axs[0,0].plot(train_re[ims[0]], label=\"pred\")\n",
    "\n",
    "    axs[0,1].plot(train_[ims[1]], label=\"train\")\n",
    "    axs[0,1].plot(train_re[ims[1]], label=\"pred\")\n",
    "\n",
    "    axs[0,2].plot(train_[ims[2]], label=\"train\")\n",
    "    axs[0,2].plot(train_re[ims[2]], label=\"pred\")\n",
    "\n",
    "    axs[0,3].plot(train_[ims[3]], label=\"train\")\n",
    "    axs[0,3].plot(train_re[ims[3]], label=\"pred\")\n",
    "\n",
    "    axs[0,4].plot(train_[ims[4]], label=\"train\")\n",
    "    axs[0,4].plot(train_re[ims[4]], label=\"pred\")\n",
    "\n",
    "\n",
    "    axs[1,0].plot(test_[ims[0]], label=\"test\")\n",
    "    axs[1,0].plot(test_re[ims[0]], label=\"pred\")\n",
    "\n",
    "    axs[1,1].plot(test_[ims[1]], label=\"test\")\n",
    "    axs[1,1].plot(test_re[ims[1]], label=\"pred\")\n",
    "\n",
    "    axs[1,2].plot(test_[ims[2]], label=\"test\")\n",
    "    axs[1,2].plot(test_re[ims[2]], label=\"pred\")\n",
    "\n",
    "    axs[1,3].plot(test_[ims[3]], label=\"test\")\n",
    "    axs[1,3].plot(test_re[ims[3]], label=\"pred\")\n",
    "\n",
    "    axs[1,4].plot(test_[ims[4]], label=\"test\")\n",
    "    axs[1,4].plot(test_re[ims[4]], label=\"pred\")\n",
    "\n",
    "\n",
    "    axs[2,0].plot(attacks_[ims[0]], label=\"attacks\")\n",
    "    axs[2,0].plot(attacks_re[ims[0]], label=\"pred\")\n",
    "\n",
    "    axs[2,1].plot(attacks_[ims[1]], label=\"attacks\")\n",
    "    axs[2,1].plot(attacks_re[ims[1]], label=\"pred\")\n",
    "\n",
    "    axs[2,2].plot(attacks_[ims[2]], label=\"attacks\")\n",
    "    axs[2,2].plot(attacks_re[ims[2]], label=\"pred\")\n",
    "\n",
    "    axs[2,3].plot(attacks_[ims[3]], label=\"attacks\")\n",
    "    axs[2,3].plot(attacks_re[ims[3]], label=\"pred\")\n",
    "\n",
    "    axs[2,4].plot(attacks_[ims[4]], label=\"attacks\")\n",
    "    axs[2,4].plot(attacks_re[ims[4]], label=\"pred\")\n",
    "    \n",
    "    for i in range(axs.shape[0]):\n",
    "        for j in range(axs.shape[1]):\n",
    "            axs[i,j].legend()\n",
    "\n",
    "def filter_signal(yn):\n",
    "    \n",
    "    \n",
    "    b, a = signal.butter(3, 0.05)\n",
    "    zi = signal.lfilter_zi(b, a)\n",
    "    z, _ = signal.lfilter(b, a, yn, zi=zi*yn[0])\n",
    "    z2, _ = signal.lfilter(b, a, z, zi=zi*z[0])\n",
    "    \n",
    "    return z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../machine_learning/cloud_functions/data-large.csv'\n",
    "data = pd.read_csv(path)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "columns = ['attack',\n",
    "           'dimension',\n",
    "           'temporal_difference-series',\n",
    "           'temporal_histogram_distance-series',\n",
    "           'temporal_gaussian_mse-series']\n",
    "\n",
    "\n",
    "df = df[columns]\n",
    "df = df.dropna()\n",
    "\n",
    "        \n",
    "series = []\n",
    "series_1 = []\n",
    "series_2 = []\n",
    "attack_ID = []\n",
    "length = 15\n",
    "\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    time_series = row['dimension'] * np.fromstring(row['temporal_histogram_distance-series'].replace('[', '').replace(']', ''), \n",
    "                                                dtype=np.float, sep=' ')[:length]\n",
    "    \n",
    "    time_series_1 = row['dimension'] * np.fromstring(row['temporal_gaussian_mse-series'].replace('[', '').replace(']', ''), \n",
    "                                            dtype=np.float, sep=' ')[:length]\n",
    "    \n",
    "    if row['attack'] == '1080p':\n",
    "        time_series_2 = np.fromstring(row['temporal_difference-series'].replace('[', '').replace(']', ''), \n",
    "                                            dtype=np.float, sep=' ')[:length]\n",
    "        time_series_2 = filter_signal(time_series_2)\n",
    "        \n",
    "    if len(time_series) < length:\n",
    "        time_series = np.append(time_series, np.zeros(length - len(time_series)))\n",
    "        \n",
    "    if len(time_series_1) < length: \n",
    "        time_series_1 = np.append(time_series_1, np.zeros(length - len(time_series_1)))\n",
    "    \n",
    "    if len(time_series_2) < length: \n",
    "        time_series_2 = np.append(time_series_2, np.zeros(length - len(time_series_2)))\n",
    "        \n",
    "    series.append(time_series)\n",
    "    series_1.append(time_series_1)\n",
    "    series_2.append(time_series_2)\n",
    "        \n",
    "    if row['attack'] in ['1080p', '720p', '480p', '360p', '240p', '144p']:\n",
    "        attack_ID.append(1)\n",
    "    else:\n",
    "        attack_ID.append(0)\n",
    "        \n",
    "df['series'] = series\n",
    "df['series_1'] = series_1\n",
    "df['series_2'] = series_2\n",
    "df['attack_ID'] = attack_ID\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "\n",
    "df_0 = df[df['attack_ID'] == False]\n",
    "df_1 = df[df['attack_ID'] == True]\n",
    "\n",
    "df_train = df_1[:int(0.8 * df_1.shape[0])]\n",
    "df_test = df_1[int(0.8 * df_1.shape[0]):]\n",
    "df_attacks = df_0\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "train = np.stack(df_train['series'].to_numpy())\n",
    "test = np.stack(df_test['series'].to_numpy())\n",
    "attacks = np.stack(df_attacks['series'].to_numpy())\n",
    "\n",
    "train_1 = np.stack(df_train['series_1'].to_numpy())\n",
    "test_1 = np.stack(df_test['series_1'].to_numpy())\n",
    "attacks_1 = np.stack(df_attacks['series_1'].to_numpy())\n",
    "\n",
    "train_2 = np.stack(df_train['series_2'].to_numpy())\n",
    "test_2 = np.stack(df_test['series_2'].to_numpy())\n",
    "attacks_2 = np.stack(df_attacks['series_2'].to_numpy())\n",
    "    \n",
    "# Free memory\n",
    "del df, df_train, df_attacks, df_0, df_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = np.copy(train)\n",
    "test_ = np.copy(test)\n",
    "attacks_ = np.copy(attacks)\n",
    "\n",
    "train_1 = np.copy(train_1)\n",
    "test_1 = np.copy(test_1)\n",
    "attacks_1 = np.copy(attacks_1)\n",
    "\n",
    "train_2 = np.copy(train_2)\n",
    "test_2 = np.copy(test_2)\n",
    "attacks_2 = np.copy(attacks_2)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "\n",
    "train_ = scaler.fit_transform(train_)\n",
    "test_ = scaler.transform(test_)\n",
    "attacks_ = scaler.transform(attacks_)\n",
    "\n",
    "\n",
    "train_1_ = scaler.fit_transform(train_1)\n",
    "test_1_ = scaler.transform(test_1)\n",
    "attacks_1_ = scaler.transform(attacks_1)\n",
    "\n",
    "\n",
    "train_2_ = scaler.fit_transform(train_2)\n",
    "test_2_ = scaler.transform(test_2)\n",
    "attacks_2_ = scaler.transform(attacks_2)\n",
    "\n",
    "train_X = np.array(train_1_)\n",
    "test_X = np.array(test_1_)\n",
    "attacks_X = np.array(attacks_1_)\n",
    "print(train_X.shape)\n",
    "train_X = train_X.reshape((train_X.shape[0], train_X.shape[1], 1))\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[1], 1))\n",
    "attacks_X = attacks_X.reshape((attacks_X.shape[0], attacks_X.shape[1], 1))\n",
    "print(train_X.shape)\n",
    "train_2_ = train_2_.reshape((train_2_.shape[0], train_2_.shape[1], 1))\n",
    "test_2_ = test_2_.reshape((test_2_.shape[0], test_2_.shape[1], 1))\n",
    "attacks_2_ = attacks_2_.reshape((attacks_2_.shape[0], attacks_2_.shape[1], 1))\n",
    "print(train_2_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='relu', input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=False))\n",
    "model.add(Dense(32))\n",
    "#model.add(RepeatVector(train_X.shape[1]))\n",
    "\n",
    "model.add(Dense(train_2_.shape[2]))\n",
    "adam = Adam(lr=0.0001)\n",
    "model.compile(optimizer=adam, loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_X, train_2_, epochs=20, verbose=1, \n",
    "                    batch_size=256, validation_data=(test_X, test_2_), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, 'AE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input into [samples, timesteps, features]\n",
    "n_in = train_X.shape[1]\n",
    "sequence = train_X.reshape((train_X.shape[0], n_in, 1))\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_in,1)))\n",
    "model.add(RepeatVector(n_in))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "adam = Adam(lr=0.0001)\n",
    "model.compile(optimizer=adam, loss='mse')\n",
    "model.summary()\n",
    "# fit model\n",
    "model.fit(sequence, sequence, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_re = model.predict(train_X, batch_size=2048)\n",
    "test_re = model.predict(test_X, batch_size=2048)\n",
    "attacks_re = model.predict(attacks_X, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = [mean_squared_error(train_X[i], train_re[i]) for i,_ in enumerate(train_X)]\n",
    "mse_test = [mean_squared_error(test_X[i], test_re[i]) for i, _ in enumerate(test_X)]\n",
    "mse_attacks = [mean_squared_error(attacks_X[i], attacks_re[i]) for i, _ in enumerate(attacks_X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(mse_train), np.mean(mse_test), np.mean(mse_attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(mse_train), np.std(mse_test), np.std(mse_attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(mse_train, mse_test, mse_attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_series(train_, train_re, test_, test_re, attacks_, attacks_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
