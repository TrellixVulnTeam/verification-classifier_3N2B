{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies as setup notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "import keras.backend as K\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_curve, auc, fbeta_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import xgboost as xgb\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import seaborn\n",
    "import time\n",
    "import pickle\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows = 999\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(network_history,title='Loss and accuracy (Keras model)'):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.subplot(211)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(network_history.history['loss'])\n",
    "    #plt.plot(network_history.history['val_loss'])\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(network_history.history['acc'])\n",
    "    #plt.plot(network_history.history['val_acc'])\n",
    "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(fpr, tpr, title):\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic {}'.format(title))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare features from generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../machine_learning/cloud_functions/data-large.csv')\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "attack_IDs = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    dimension = int(row['attack'].split('_')[0].replace('p',''))\n",
    "\n",
    "    if row['attack'] in ['1080p', '720p', '480p', '360p', '240p', '144p']:\n",
    "            attack_IDs.append(1)\n",
    "    else:\n",
    "        attack_IDs.append(0)\n",
    "\n",
    "df['attack_ID'] = attack_IDs\n",
    "\n",
    "df = df.drop(['Unnamed: 0',\n",
    "              'attack',\n",
    "              'title',\n",
    "              'path',\n",
    "              'kind',\n",
    "             'temporal_canny-series',\n",
    "             'temporal_cross_correlation-series',\n",
    "             'temporal_difference-series',\n",
    "             'temporal_histogram_distance-series', \n",
    "             'temporal_dct-series', \n",
    "        ],axis=1)\n",
    "df=df.dropna(axis=1)\n",
    "print('Number of attacks:',df[df['attack_ID']==0].shape)\n",
    "print('Number of legit renditions:',df[df['attack_ID']==1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "corr = df_corr.drop(['attack_ID'],axis=1).corr('spearman')\n",
    "corr.style.background_gradient().set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the input of our models (we try to make balanced dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(df.shape[0]*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training set as 80% of total specimens\n",
    "num_train = int(df.shape[0]*0.8)\n",
    "\n",
    "df_train_all = df[0:num_train]\n",
    "df_test_all = df[num_train:]\n",
    "print('We have {} train specimens and {} test specimens'.format(len(df_train_all), len(df_test_all)))\n",
    "\n",
    "# Balance the training dataset by limiting the number of negative specimens to resemble what we have of positive\n",
    "df_train_1 = df_train_all[df_train_all['attack_ID'] == 1]\n",
    "df_train_0 = df_train_all[df_train_all['attack_ID'] == 0]\n",
    "\n",
    "print('We have {} positive training specimens and {} negative training specimens'.format(len(df_train_1), len(df_train_0)))\n",
    "# Take a sample from the training positives and build the final training set\n",
    "df_sample_train = df_train_0.sample(df_train_1.shape[0])\n",
    "df_train = df_train_1.append(df_sample_train)\n",
    "df_train = df_train.sample(frac=1)\n",
    "print('Balanced training set established with shape {}'.format(df_train.shape))\n",
    "\n",
    "X_test_all = df_test_all.drop(['attack_ID'],axis=1)\n",
    "df_test_1 = df_test_all[df_test_all['attack_ID'] == 1]\n",
    "df_test_0 = df_test_all[df_test_all['attack_ID'] == 0]\n",
    "\n",
    "print('We have {} positive testing specimens and {} negative testing specimens'.format(len(df_test_1), len(df_test_0)))\n",
    "# Get another sample from the testing positives and build the final test set\n",
    "df_sample_test = df_test_0.sample(df_test_0.shape[0])\n",
    "df_test = df_test_1.append(df_sample_test)\n",
    "df_test = df_test.sample(frac=0.6)\n",
    "print('Balanced test set established with shape {}'.format(df_test.shape))\n",
    "\n",
    "X_test_all = np.asarray(X_test_all)\n",
    "\n",
    "y_test_all = df_test_all['attack_ID']\n",
    "y_test_all = np.asarray(y_test_all)\n",
    "\n",
    "X_train = df_train.drop(['attack_ID'],axis=1)\n",
    "X_train = np.asarray(X_train)\n",
    "\n",
    "X_test = df_test.drop(['attack_ID'],axis=1)\n",
    "X_test = np.asarray(X_test)\n",
    "\n",
    "y_train = df_train['attack_ID']\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "y_test = df_test['attack_ID']\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a model where we take all variables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TRAIN:', X_train.shape)\n",
    "print('TEST:', X_test.shape)\n",
    "print(df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can normalize input data to facilitate model convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinMax_scaler = MinMaxScaler()\n",
    "Standard_scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled_MinMax = MinMax_scaler.fit_transform(X_train) \n",
    "X_test_scaled_MinMax = MinMax_scaler.transform(X_test) \n",
    "X_test_scaled_MinMax_all = MinMax_scaler.transform(X_test_all) \n",
    "\n",
    "X_train_scaled_standard = Standard_scaler.fit_transform(X_train)\n",
    "X_test_scaled_standard = Standard_scaler.transform(X_test)\n",
    "X_test_scaled_standard_all = Standard_scaler.transform(X_test_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define models\n",
    "We will explore results with different ML techniques\n",
    "\n",
    "## Keras neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(100, input_shape=(X_train.shape[1],), activation= \"relu\", kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(100, activation= \"relu\", kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, kernel_initializer='glorot_uniform', activation='sigmoid'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, kernel_initializer='glorot_uniform', activation= \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, kernel_initializer='glorot_uniform', activation= \"relu\"))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(256, kernel_initializer='glorot_uniform'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    network_history = model.fit(X_train_scaled_standard, y_train, batch_size=128, epochs=500, verbose=0, validation_data=(X_test_scaled_standard,y_test))\n",
    "    plot_history(network_history)\n",
    "    return model\n",
    "\n",
    "NN_model = model()\n",
    "print(NN_model.metrics_names)\n",
    "\n",
    "NN_model.evaluate(X_test_scaled_standard, y_test)\n",
    "\n",
    "# Save the weights\n",
    "NN_model.save_weights('../output/models/NN_model_weights.h5')\n",
    "\n",
    "# Save the model architecture\n",
    "with open('../output/models/model_architecture.json', 'w') as f:\n",
    "    f.write(NN_model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = NN_model.predict(X_test_scaled_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rounded = [round(x[0]) for x in y_pred]\n",
    "y_pred_bin = np.array(rounded, dtype='int64')\n",
    "confusion_matrix(y_test, y_pred_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, y_pred_bin)\n",
    "fb = fbeta_score(y_test, y_pred_bin, beta=20, pos_label=1) \n",
    "print('TNR={}, TPR={}, F20={}'.format(1-fpr[1],tpr[1], fb))\n",
    "plot_roc(fpr, tpr, 'Neural network')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test only with Class : 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_RF = X_train_scaled_MinMax\n",
    "X_test_RF = X_test_scaled_MinMax\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=150, random_state=12)\n",
    "random_forest.fit(X_train_RF, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest.score(X_test_RF, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_RF = random_forest.predict(X_test_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, prediction_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, prediction_RF)\n",
    "fb = fbeta_score(y_test, prediction_RF, beta=20, pos_label=1) \n",
    "print('TNR={}, TPR={}, F20={}'.format(1-fpr[1],tpr[1], fb))\n",
    "plot_roc(fpr, tpr, 'Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export model to use in CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to file\n",
    "pickle.dump(random_forest, open(\"../output/models/random_forest.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_AB = X_train_scaled_MinMax\n",
    "X_test_AB = X_test_scaled_MinMax\n",
    "\n",
    "AdaBoost = AdaBoostClassifier(learning_rate=0.55, random_state=3)\n",
    "AdaBoost.fit(X_train_AB, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoost.score(X_test_AB, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_AdaBoost = AdaBoost.predict(X_test_AB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, prediction_AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, prediction_AdaBoost)\n",
    "fb = fbeta_score(y_test, prediction_AdaBoost, beta=20,pos_label=1) \n",
    "print('TNR={}, TPR={}, F20={}'.format(1-fpr[1],tpr[1], fb))\n",
    "plot_roc(fpr, tpr, 'AdaBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AdaBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export model to use in CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to file\n",
    "pickle.dump(AdaBoost, open(\"../output/models/AdaBoost.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_SVM = X_train_scaled_standard\n",
    "X_test_SVM = X_test_scaled_standard_all\n",
    "Y_test_SVM = y_test_all\n",
    "\n",
    "classifier = svm.SVC(kernel='rbf')\n",
    "classifier.fit(X_train_SVM, y_train)\n",
    "prediction_SVM = classifier.predict(X_test_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.score(X_test_SVM,Y_test_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test_SVM, prediction_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(Y_test_SVM, prediction_SVM)\n",
    "fb = fbeta_score(Y_test_SVM, prediction_SVM, beta=20, pos_label=1) \n",
    "print('TNR={}, TPR={}, F20={}'.format(1-fpr[1],tpr[1], fb))\n",
    "plot_roc(fpr, tpr, 'SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_XGB = X_train\n",
    "X_test_XGB = X_test_all\n",
    "Y_test_XGB = y_test_all\n",
    "\n",
    "XGBoost = xgb.XGBClassifier()\n",
    "grid = {'max_depth':10}\n",
    "XGBoost.set_params(**grid)\n",
    "XGBoost.fit(X_train_XGB, y_train)\n",
    "prediction_XGB = XGBoost.predict(X_test_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost.score(X_test_XGB,Y_test_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(Y_test_XGB, prediction_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(Y_test_XGB, prediction_XGB)\n",
    "fb = fbeta_score(y_test_all, prediction_XGB, beta=20, pos_label=1) \n",
    "print('TNR={}, TPR={}, F20={}'.format(1-fpr[1],tpr[1], fb))\n",
    "plot_roc(fpr, tpr, 'XGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export model to use in CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to file\n",
    "pickle.dump(XGBoost, open(\"../output/models/XGBoost.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
