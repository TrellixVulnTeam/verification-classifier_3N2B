{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import random_projection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import fbeta_score, roc_curve, auc\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as offline\n",
    "from plotly import tools\n",
    "\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "pd.options.display.max_rows = 999\n",
    "\n",
    "sys.path.insert(0, '../../scripts/modeling_toolbox/')\n",
    "# load the autoreload extension\n",
    "%load_ext autoreload\n",
    "# Set extension to reload modules every time before executing code\n",
    "%autoreload 2\n",
    "\n",
    "from metric_processor import MetricProcessor\n",
    "import evaluation\n",
    "\n",
    "%matplotlib inline\n",
    "offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['temporal_dct-mean', 'temporal_gaussian-mean', 'temporal_spatial_complexity-mean',\n",
    "           'temporal_difference-mean', 'dimension', 'temporal_gaussian_difference-mean']\n",
    "\n",
    "\n",
    "path = '../../machine_learning/cloud_functions/new-attacks-data-large.csv'\n",
    "\n",
    "metric_processor = MetricProcessor(features,'UL', path)\n",
    "df = metric_processor.read_and_process_data()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, X_attacks), (df_train, df_test, df_attacks) = metric_processor.split_test_and_train(df)\n",
    "\n",
    "print('Shape of train: {}'.format(X_train.shape))\n",
    "print('Shape of test: {}'.format(X_test.shape))\n",
    "print('Shape of attacks: {}'.format(X_attacks.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "ss = StandardScaler()\n",
    "x_train = ss.fit_transform(X_train)\n",
    "x_test = ss.transform(X_test)\n",
    "x_attacks = ss.transform(X_attacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OCSVM = svm.OneClassSVM(kernel='rbf',gamma='auto', nu=0.01, cache_size=5000)\n",
    "OCSVM.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb, area, tnr, tpr_train, tpr_test = evaluation.unsupervised_evaluation(OCSVM, x_train, x_test, x_attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TNR: {}\\nTPR_test: {}\\nTPR_train: {}\\n'.format(tnr, tpr_test, tpr_train))\n",
    "print('F20: {}\\nAUC: {}'.format(fb, area))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Distances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = OCSVM.decision_function(x_train)\n",
    "test_scores = OCSVM.decision_function(x_test)\n",
    "attack_scores = OCSVM.decision_function(x_attacks)\n",
    "\n",
    "print('Mean score values:\\n-Train: {}\\n-Test: {}\\n-Attacks: {}'.format(np.mean(train_scores),\n",
    "                                                                       np.mean(test_scores),\n",
    "                                                                       np.mean(attack_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace0 = go.Box(\n",
    "    y=test_scores,\n",
    "    name='test'\n",
    "    \n",
    ")\n",
    "trace1 = go.Box(\n",
    "    y=attack_scores,\n",
    "    name='attacks'\n",
    ")\n",
    "data = [trace0, trace1]\n",
    "\n",
    "layout = {'title': 'Boxplots', \n",
    "          'yaxis': {'title': 'Distance to decision function'}\n",
    "         }\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative distances mean points outside the decision function thus, classified as attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will study the distances to the decision function comparing them to different attacks and resolutions, in order to gain insights of the model we have built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, df_attacks = df_train.reset_index(), df_test.reset_index(), df_attacks.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['distance_to_dec_func'] = train_scores\n",
    "df_test['distance_to_dec_func'] = test_scores\n",
    "df_attacks['distance_to_dec_func'] = attack_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolutions = df_test['dimension'].unique()\n",
    "attacks = df_attacks['attack'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "resolutions = np.sort(resolutions)\n",
    "for res in resolutions:\n",
    "    selection = df_test[df_test['dimension'] == res]\n",
    "    trace = go.Box(y = selection['distance_to_dec_func'], name = str(res) + 'p',\n",
    "                   text = selection['title']\n",
    ")\n",
    "    data.append(trace)\n",
    "\n",
    "layout = go.Layout(\n",
    "            title=go.layout.Title(text='Test Set'),\n",
    "            yaxis = go.layout.YAxis(title = 'Distance to decision function'),\n",
    "            xaxis = go.layout.XAxis(\n",
    "                title = 'Resolutions',\n",
    "                tickmode = 'array',\n",
    "                ticktext = [str(i) + 'p' for i in resolutions]\n",
    "            )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "resolutions = np.sort(resolutions)\n",
    "for res in resolutions:\n",
    "    selection = df_attacks[df_attacks['dimension'] == res]\n",
    "    trace = go.Box(y = selection['distance_to_dec_func'], name = str(res) + 'p',\n",
    "                   text = selection['title']\n",
    ")\n",
    "    data.append(trace)\n",
    "\n",
    "layout = go.Layout(\n",
    "            title=go.layout.Title(text='Attack Set'),\n",
    "            yaxis = go.layout.YAxis(title = 'Distance to decision function'),\n",
    "            xaxis = go.layout.XAxis(\n",
    "                title = 'Resolutions',\n",
    "                tickmode = 'array',\n",
    "                ticktext = [str(i) + 'p' for i in resolutions]\n",
    "            )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "attack_types = list(set([i.split('_', 1)[1] for i in attacks]))\n",
    "for attk in attack_types:\n",
    "    selection = df_attacks[df_attacks['attack'].str.contains(attk)]\n",
    "    trace = go.Box(y = selection['distance_to_dec_func'], name = attk, text = selection['title'])\n",
    "    data.append(trace)\n",
    "\n",
    "layout = go.Layout(\n",
    "            title=go.layout.Title(text='Attack Set'),\n",
    "            yaxis = go.layout.YAxis(title = 'Distance to decision function'),\n",
    "            xaxis = go.layout.XAxis(\n",
    "                title = 'Attack Type',\n",
    "                tickmode = 'array',\n",
    "                ticktext = attack_types\n",
    "            )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for res in resolutions:\n",
    "    for attk in attack_types:\n",
    "        selection = df_attacks[(df_attacks['attack'].str.contains(attk)) & (df_attacks['dimension'] == res)]\n",
    "        trace = go.Box(y = selection['distance_to_dec_func'], name = '{}p-{}'.format(res,attk),\n",
    "        text = selection['title'])\n",
    "        data.append(trace)\n",
    "\n",
    "\n",
    "\n",
    "    layout = go.Layout(\n",
    "            title=go.layout.Title(text=str(res)+ 'p'),\n",
    "            yaxis = go.layout.YAxis(title = 'Distance to decision function'),\n",
    "            xaxis = go.layout.XAxis(\n",
    "                title = 'Attack Type',\n",
    "                tickmode = 'array',\n",
    "                ticktext = attack_types\n",
    "            )\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    offline.iplot(fig)\n",
    "    data = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import BernoulliRBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rbm = BernoulliRBM(n_components=2, verbose=1, batch_size=1024, learning_rate=0.00004, n_iter=1000) # -3.35\n",
    "rbm = BernoulliRBM(n_components=2, verbose=1, batch_size=1024, learning_rate=0.00004, n_iter=1000)\n",
    "\n",
    "rbm.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_train = rbm.transform(x_train)\n",
    "latent_test = rbm.transform(x_test)\n",
    "latent_attacks = rbm.transform(x_attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(latent_train, axis=0), np.mean(latent_test, axis=0), np.mean(latent_attacks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = 0.2\n",
    "# train_pred = [x > th and y > th for x,y in latent_train]\n",
    "# test_pred = [x > th and y > th for x,y in latent_test]\n",
    "# attacks_pred = [x > th and y > th for x,y in latent_attacks]\n",
    "\n",
    "train_pred = [np.sum(i) > th for i in latent_train]\n",
    "test_pred = [np.sum(i) > th for i in latent_test]\n",
    "attacks_pred = [np.sum(i) > th for i in latent_attacks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train TPR: {}'.format(sum(train_pred)/len(train_pred)))\n",
    "print('Test TPR: {}'.format(sum(test_pred)/len(test_pred)))\n",
    "print('TNR: {}'.format((len(attacks_pred) - sum(attacks_pred))/len(attacks_pred)))\n",
    "\n",
    "true_positives = sum(test_pred)\n",
    "false_negatives = len(test_pred) - true_positives\n",
    "false_positives = sum(attacks_pred)\n",
    "true_negatives = len(attacks_pred) - false_positives\n",
    "\n",
    "beta = 20\n",
    "precision = true_positives/(true_positives+false_positives)\n",
    "recall = true_positives/(true_positives+false_negatives)\n",
    "F20 = (1 + (beta ** 2))*precision*recall/((beta ** 2)*precision + recall)\n",
    "print('F20: {}'.format(F20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_train = rbm.score_samples(x_train)\n",
    "score_test = rbm.score_samples(x_test)\n",
    "score_attacks = rbm.score_samples(x_attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(score_train), np.mean(score_test), np.mean(score_attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = np.quantile(score_train, 0.99)\n",
    "\n",
    "\n",
    "print('Thresholding the 99% quantile')\n",
    "print('Train TPR: {}'.format(1 - sum(score_train > th) / len(score_train)))\n",
    "print('Test TPR: {}'.format(1 - sum(score_test > th) / len(score_test)))\n",
    "print('TNR: {}'.format(1 - sum(score_attacks < th) / len(score_attacks)))\n",
    "\n",
    "true_positives = sum(score_test < th)\n",
    "false_negatives = sum(score_test > th)\n",
    "false_positives = sum(score_attacks < th)\n",
    "true_negatives = sum(score_attacks > th)\n",
    "\n",
    "beta = 20\n",
    "precision = true_positives/(true_positives+false_positives)\n",
    "recall = true_positives/(true_positives+false_negatives)\n",
    "F20 = (1 + (beta ** 2))*precision*recall/((beta ** 2)*precision + recall)\n",
    "print('F20: {}'.format(F20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(latent_attacks[:100,0], latent_attacks[:100,1])\n",
    "plt.scatter(latent_train[:100,0], latent_train[:100,1])\n",
    "plt.scatter(latent_test[:100,0], latent_test[:100,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "OCSVM = svm.OneClassSVM(kernel='linear',gamma='auto', nu=0.01, cache_size=5000)\n",
    "\n",
    "OCSVM.fit(latent_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb, area, tnr, tpr_train, tpr_test = evaluation.unsupervised_evaluation(OCSVM, latent_train, latent_test, latent_attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TNR: {}\\nTPR_test: {}\\nTPR_train: {}\\n'.format(tnr, tpr_test, tpr_train))\n",
    "print('F20: {}\\nAUC: {}'.format(fb, area))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_history_loss(history):\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "plot_train_history_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "batch_size = 512\n",
    "original_dim = x_train.shape[1]\n",
    "latent_dim = 2\n",
    "intermediate_dim = 2\n",
    "epochs = 50\n",
    "epsilon_std = 1.0\n",
    "\n",
    "\n",
    "x = Input(shape=(original_dim,))\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# to reuse these later\n",
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "# instantiate VAE model\n",
    "vae = Model(x, x_decoded_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VAE loss\n",
    "xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "vae_loss = K.mean(xent_loss + kl_loss)\n",
    "\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = vae.fit(x_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, None))\n",
    "\n",
    "plot_train_history_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model to project inputs on the latent space\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "y_test = df_test['dimension']\n",
    "\n",
    "# display a 2D plot of the digit classes in the latent space\n",
    "def plot_latentSpace(encoder, x_test, y_test, batch_size):\n",
    "    x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test, cmap='tab10')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "plot_latentSpace(encoder, x_test, y_test, batch_size)\n",
    "test_latent = encoder.predict(x_test, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['dimension']\n",
    "plot_latentSpace(encoder, x_train, y_train, batch_size)\n",
    "train_latent = encoder.predict(x_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_attacks = df_attacks['dimension']\n",
    "plot_latentSpace(encoder, x_attacks, y_attacks, batch_size)\n",
    "attacks_latent = encoder.predict(x_attacks, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "OCSVM = svm.OneClassSVM(kernel='rbf',gamma='auto', nu=0.01, cache_size=5000)\n",
    "\n",
    "OCSVM.fit(train_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb, area, tnr, tpr_train, tpr_test = evaluation.unsupervised_evaluation(OCSVM, train_latent, test_latent, attacks_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TNR: {}\\nTPR_test: {}\\nTPR_train: {}\\n'.format(tnr, tpr_test, tpr_train))\n",
    "print('F20: {}\\nAUC: {}'.format(fb, area))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minisom import MiniSom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som = MiniSom(10, 10, 6, sigma=1, learning_rate=0.05) # initialization of 6x6 SOM\n",
    "som.train_batch(x_train, 100) # trains the SOM with 100 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_train = np.linalg.norm(som.quantization(x_train) - x_train, axis=1)\n",
    "score_test = np.linalg.norm(som.quantization(x_test) - x_test, axis=1)\n",
    "score_attacks = np.linalg.norm(som.quantization(x_attacks) - x_attacks, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(score_train), np.mean(score_test), np.mean(score_attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = np.quantile(score_train, 0.99)\n",
    "\n",
    "\n",
    "print('Thresholding the 99% quantile')\n",
    "print('Train TPR: {}'.format(1 - sum(score_train > th) / len(score_train)))\n",
    "print('Test TPR: {}'.format(1 - sum(score_test > th) / len(score_test)))\n",
    "print('TNR: {}'.format(1 - sum(score_attacks < th) / len(score_attacks)))\n",
    "\n",
    "true_positives = sum(score_test < th)\n",
    "false_negatives = sum(score_test > th)\n",
    "false_positives = sum(score_attacks < th)\n",
    "true_negatives = sum(score_attacks > th)\n",
    "\n",
    "beta = 20\n",
    "precision = true_positives/(true_positives+false_positives)\n",
    "recall = true_positives/(true_positives+false_negatives)\n",
    "F20 = (1 + (beta ** 2))*precision*recall/((beta ** 2)*precision + recall)\n",
    "print('F20: {}'.format(F20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Isomap(n_components=2, n_jobs=7)\n",
    "embedding.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_train = embedding.transform(x_train)\n",
    "latent_test = embedding.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_attacks = embedding.transform(x_attacks[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(latent_train[:, 0], latent_train[:, 1])\n",
    "plt.scatter(latent_test[:, 0], latent_test[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(latent_attacks[:, 0], latent_attacks[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
